{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dcd186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shubham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3b7782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Dataset\n",
    "data = pd.read_csv(\"mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b335de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ed58c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'who', 'don', 'ma', 'further', 'herself', 'only', 'am', 'were', 'all', 'those', 'shouldn', 'was', 'into', 'too', \"won't\", 'same', 'isn', 'while', 'such', 'over', \"didn't\", 'an', 'itself', 'mightn', 'yourselves', \"mustn't\", 'themselves', 'me', 'because', \"needn't\", 'd', 'can', 'these', 'the', 'how', 'their', 'below', 'under', 're', 'about', \"couldn't\", 'which', 'll', 'your', \"haven't\", \"should've\", 'did', 'more', 'nor', 'from', 'of', 'own', 'shan', 'again', \"doesn't\", 'you', 'hers', 'having', 'but', 'against', 's', 'out', 'have', 'some', 'and', 'theirs', 'had', 'there', 'a', 'myself', 'just', 'are', 'ourselves', \"wouldn't\", 'she', 'by', \"you'll\", 'didn', 'is', 'do', 'off', 'won', \"you've\", 'been', 'now', 'weren', 'here', \"weren't\", 'during', \"that'll\", 'doing', 'where', 'i', 'down', \"shan't\", 'yours', 'what', \"don't\", 'will', 'doesn', \"hadn't\", 'his', 'few', \"wasn't\", 'y', 'both', 'yourself', 'this', \"you'd\", 'any', 'at', 'for', 'after', \"you're\", 'aren', 'ain', 'not', 'wasn', 'its', 'them', \"shouldn't\", 'him', 'be', 'does', 'm', 'whom', 'or', 'o', 'with', 'he', 'hasn', 'up', 'they', 'above', 'no', 'before', 'each', 'to', 'being', \"aren't\", \"mightn't\", \"hasn't\", 'her', 'through', 'wouldn', 'very', 'that', 'haven', 'needn', 'mustn', 'in', \"she's\", 'our', 'between', \"isn't\", 'why', 'couldn', 'other', 'himself', 't', 'we', 'once', 'most', 'should', 'my', 'so', 'if', 'it', 'hadn', \"it's\", 'than', 've', 'on', 'until', 'then', 'when', 'as', 'ours', 'has'}\n",
      "0       http// intj moment https// sportscenter top te...\n",
      "1       Im finding lack post alarming.Sex boring posit...\n",
      "2       Good one https// course say know thats blessin...\n",
      "3       Dear INTP enjoyed conversation day. Esoteric g...\n",
      "4       Youre fired.Thats another silly misconception....\n",
      "                              ...                        \n",
      "8670    https// always think cat Fi doms reason. https...\n",
      "8671    ... thread already exists someplace else doe h...\n",
      "8672    many question things. would take purple pill. ...\n",
      "8673    conflicted right come wanting children. honest...\n",
      "8674    ha long since personalitycafe although doesnt ...\n",
      "Name: posts, Length: 8675, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Regular expression to match URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Replace URLs with an empty string\n",
    "    clean_text = url_pattern.sub('', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "#Removing the square brackets and notations\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('[^A-Za-z0-9/. ]', '', text)\n",
    "\n",
    "#Lemmatizing the text\n",
    "def simple_lemmatizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "    \n",
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "#Apply function on review column\n",
    "data['posts']=data['posts'].apply(remove_urls)\n",
    "data['posts']=data['posts'].apply(strip_html)\n",
    "data['posts']=data['posts'].apply(remove_between_square_brackets)\n",
    "#data['posts']=data['posts'].apply(remove_notations)\n",
    "data['posts']=data['posts'].apply(simple_lemmatizer)\n",
    "data['posts']=data['posts'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b95c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data\n",
    "data.to_csv(\"data_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d0c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
